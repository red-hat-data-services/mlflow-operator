# MLflow Operator

Kubernetes operator for managing MLflow deployments.

## Project Structure

This project was generated using [Kubebuilder](https://book.kubebuilder.io/) v4.10.1.

## Resources

### MLflow (mlflow.opendatahub.io/v1)

The MLflow custom resource is **cluster-scoped**, meaning it can be created without specifying a namespace and is accessible across the entire cluster.

## API Definitions

The `api/` folder contains the API type definitions for all custom resources:

```
api/
└── v1/
    ├── groupversion_info.go    # API group and version registration
    ├── mlflow_types.go          # MLflow resource type definitions
    └── zz_generated.deepcopy.go # Auto-generated DeepCopy methods
```

### Modifying API Types

To add or modify fields in the MLflow resource:

1. Edit `api/v1/mlflow_types.go`
   - Add fields to `MLflowSpec` for desired state
   - Add fields to `MLflowStatus` for observed state
   - Use Kubebuilder markers for validation, defaults, and CRD generation

2. Regenerate code and manifests:
   ```bash
   make manifests generate
   ```

3. The CRD will be updated at `config/crd/bases/mlflow.opendatahub.io_mlflows.yaml`

**Important**: Never manually edit `zz_generated.deepcopy.go` - it's automatically generated by `make generate`.

> Note: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

## Development

Generated code and manifests are managed through Kubebuilder's code generation tools:

```bash
# Generate CRD manifests, RBAC, and webhooks
make manifests

# Generate Go code (DeepCopy, DeepCopyInto, DeepCopyObject)
make generate
```

**Note**: Always regenerate manifests and code after modifying API types. CI will verify that generated code is up-to-date.

## Testing

### Unit Tests

Unit tests are located in `internal/controller/` and can be run with:

```bash
make test
```

### E2E Tests

End-to-end tests are located in `test/e2e/` and require a Kind cluster:

```bash
make test-e2e
```

This will automatically create a Kind cluster, run the tests, and clean up the cluster afterwards.
