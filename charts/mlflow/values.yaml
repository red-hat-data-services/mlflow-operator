# Default values for mlflow
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Namespace where MLflow will be deployed
namespace: mlflow

# Resource suffix for unique naming across multiple MLflow instances
# All resources will be named like "mlflow{{ .Values.resourceSuffix }}"
# Example: If resourceSuffix is "-dev", resources will be named "mlflow-dev"
# Leave empty (default) for standard "mlflow" resource names
# This is automatically set by the operator based on the CR name
resourceSuffix: ""

# Common labels applied to all resources (Service, Deployment, ServiceAccount, etc.)
commonLabels:
  component: mlflow

# Pod-specific labels applied only to the MLflow pod
# Use this for pod-specific metadata like version, environment, etc.
# For labels that should be on all resources, use commonLabels
podLabels: {}

# TLS configuration for the MLflow server (handled directly by uvicorn)
tls:
  # Secret containing TLS certificate and key (tls.crt, tls.key)
  # On OpenShift, this is automatically generated via the service-ca annotation
  secretName: mlflow-tls

# MLflow deployment configuration
replicaCount: 1

image:
  name: quay.io/opendatahub/mlflow:latest
  # imagePullPolicy: IfNotPresent  # Optional: Override k8s defaults (IfNotPresent for most images, Always for :latest)

serviceAccount:
  name: mlflow-sa

# Resources for MLflow container
resources:
  requests:
    cpu: "1"
    memory: 2Gi
  limits:
    cpu: "4"
    memory: 3Gi

# Persistent storage
# Only required if using file-based or SQLite backend/registry stores or file-based artifacts.
# Set false when using remote storage (S3, PostgreSQL, etc.)
storage:
  enabled: true  # Enable by default - disable for remote file-based storage
  size: 2Gi
  storageClassName: ""  # Use default storage class
  accessMode: ReadWriteOnce

# MLflow server configuration
mlflow:
  # Backend store URI (where MLflow stores experiment and run metadata)
  # Default uses SQLite which requires storage.enabled: true
  # For production, use remote DB: postgresql://user:pass@host:5432/mlflow
  # Note: For URIs containing credentials, use backendStoreUriFrom instead
  backendStoreUri: "sqlite:////mlflow/mlflow.db"

  # Backend store URI from secret (preferred for URIs with credentials)
  # Takes precedence over backendStoreUri if both are set
  # Example:
  #   backendStoreUriFrom:
  #     secretKeyRef:
  #       name: mlflow-db-credentials
  #       key: backend-store-uri
  #       optional: false
  backendStoreUriFrom: {}

  # Registry store URI (where MLflow stores model registry metadata)
  # If empty, uses the same value as backendStoreUri
  # Note: For URIs containing credentials, use registryStoreUriFrom instead
  # registryStoreUri: "sqlite:////mlflow/mlflow.db"

  # Registry store URI from secret (preferred for URIs with credentials)
  # Takes precedence over registryStoreUri if both are set
  # Example:
  #   registryStoreUriFrom:
  #     secretKeyRef:
  #       name: mlflow-db-credentials
  #       key: registry-store-uri
  #       optional: false
  # registryStoreUriFrom: {}

  # Artifacts destination (where MLflow stores artifacts)
  # Only used when serveArtifacts is enabled. When serveArtifacts is disabled,
  # this value is ignored and clients access artifact storage directly.
  # Default uses local files which requires storage.enabled: true
  # For production, use remote storage: s3://bucket/path or gs://bucket/path
  artifactsDestination: "file:///mlflow/artifacts"

  # Default artifact root path for MLflow runs
  # This is used when a run doesn't specify an artifact location.
  # If not specified, defaults to artifactsDestination value.
  # Supported schemes: file://, s3://, gs://, wasbs://, hdfs://, etc.
  # Example: s3://my-bucket/mlflow/artifacts
  # defaultArtifactRoot: ""

  # Enable workspaces
  enableWorkspaces: true
  # Workspace store URI
  workspaceStoreUri: "kubernetes://"
  # Enable artifact serving
  # When enabled, adds the --serve-artifacts flag to the MLflow server and uses artifactsDestination
  # to configure where artifacts are stored. This allows clients to log and retrieve artifacts
  # through the MLflow server's REST API instead of directly accessing the artifact storage.
  # When disabled, artifactsDestination is ignored and clients must have direct access to artifact storage.
  # REQUIRED when using file-based artifact storage (defaultArtifactRoot starting with "file://").
  serveArtifacts: true
  # Number of gunicorn worker processes for the MLflow server
  # Note: This is different from pod replicas. Each pod will run this many worker processes.
  # Defaults to 1. For high-traffic deployments, consider increasing pod replicas instead.
  workers: 1
  # Port for MLflow server
  port: 8443
  # Allowed hosts (will be generated based on routes/services)
  allowedHosts: []
  # Static URL prefix for MLflow (e.g., "/mlflow" for proxied deployments)
  # This also gets set as the --root-path for uvicorn in the deployment template.
  # Leave empty for direct access without a prefix
  staticPrefix: ""

# Environment variables for MLflow container
# Supports both direct values and references to secrets/configmaps
env:
  - name: MLFLOW_LOGGING_LEVEL
    value: INFO
  # To override defaults, add env entries here. MLFLOW_K8S_AUTH_AUTHORIZATION_MODE
  # is set to self_subject_access_review by default; add an entry below to change it.

# Additional environment variables from secrets/configmaps
envFrom: []
# Example:
# - secretRef:
#     name: my-secret
# - configMapRef:
#     name: my-configmap

# Security context for pod
podSecurityContext:
  runAsNonRoot: true
  seccompProfile:
    type: RuntimeDefault

# Security context for containers
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false

# Service configuration
service:
  type: ClusterIP
  port: 8443
  # Annotations to add to the service
  annotations: {}

# Node selector, tolerations, and affinity
nodeSelector: {}
tolerations: []
affinity: {}

# CA Bundle configuration for TLS verification
# User-provided CA bundle from a ConfigMap
caBundleConfigMap:
  enabled: false
  name: ""
  key: ""

# Platform CA Bundle (e.g., ODH/RHOAI trusted CA bundle)
# This is automatically detected and mounted by the operator if present
# The ConfigMap is expected to have keys: "ca-bundle.crt" and/or "odh-ca-bundle.crt"
# Ref: https://github.com/opendatahub-io/architecture-decision-records/pull/28
platformCABundle:
  enabled: false  # Set to true by operator when ConfigMap is detected
  configMapName: odh-trusted-ca-bundle
  volumeName: platform-ca-bundle
  mountPath: /etc/pki/tls/certs/platform
  filePath: /etc/pki/tls/certs/platform/ca-bundle.crt        # Main CA bundle
  extraFilePath: /etc/pki/tls/certs/platform/odh-ca-bundle.crt  # Additional platform certs

# CA Bundle configuration
# When enabled, an init container creates a single PEM file that includes:
# - System CA certificates from the base image (/etc/pki/tls/certs/ca-bundle.crt)
# - Platform CA bundle (if present)
# - User-provided custom CA bundle (if configured)
# This file is used by all Python libraries (boto3, requests, psycopg2, etc.)
caBundle:
  enabled: false  # Set to true by operator when any CA source is configured
  mountPath: /etc/pki/tls/certs/combined
  filePath: /etc/pki/tls/certs/combined/ca-bundle.crt
  systemBundlePath: /etc/pki/tls/certs/ca-bundle.crt
  # Seconds between checks for CA bundle ConfigMap changes
  # A sidecar container watches for changes and regenerates the bundle in-place
  watchInterval: 30
